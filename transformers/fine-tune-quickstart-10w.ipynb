{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "DATASET_PATH= \"/mnt/data/huggingface/datasets\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset = load_dataset(f\"{DATASET_PATH}/yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['label', 'text'],\n        num_rows: 650000\n    })\n    test: Dataset({\n        features: ['label', 'text'],\n        num_rows: 50000\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'label': 2,\n 'text': \"As far as Starbucks go, this is a pretty nice one.  The baristas are friendly and while I was here, a lot of regulars must have come in, because they bantered away with almost everyone.  The bathroom was clean and well maintained and the trash wasn't overflowing in the canisters around the store.  The pastries looked fresh, but I didn't partake.  The noise level was also at a nice working level - not too loud, music just barely audible.\\\\n\\\\nI do wish there was more seating.  It is nice that this location has a counter at the end of the bar for sole workers, but it doesn't replace more tables.  I'm sure this isn't as much of a problem in the summer when there's the space outside.\\\\n\\\\nThere was a treat receipt promo going on, but the barista didn't tell me about it, which I found odd.  Usually when they have promos like that going on, they ask everyone if they want their receipt to come back later in the day to claim whatever the offer is.  Today it was one of their new pastries for $1, I know in the summer they do $2 grande iced drinks with that morning's receipt.\\\\n\\\\nOverall, nice working or socializing environment.  Very friendly and inviting.  It's what I've come to expect from Starbucks, so points for consistency.\"}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3 stars</td>\n      <td>Okay, so I took my baby to Good Night Pediatrics recently and so this was the closest 24hr pharmacy that took my insurance...so yeah. I haven't actually ever been in this store, So I really can't review on that. BUT, I have gone through the drive thru for the pharmacy...at first I was going to give this CVS a two stars, BECAUSE the first associate and pharmacist that helped me, I wasn't really impressed and when I first picked up my prescriptions, the pharmacist usually needs to review the rx's with you, but the chick just handed me the bag, said,\\\"have any questions?\\\"..LOL...but I did a tard move recently....I administered my baby her first dose, had my 6 yr old close the bottle and place in the refrigerator... then 12 hrs later went to give my baby another dose...DIDN'T check the cap...shook the bottle, THEN got that awesome pink, bubble gum scented amoxicillin all over myself, my baby, the carpet, and couch....AWESOME! I know :) HA!\\nSo, I just called over to the pharmacy this morning and the pharmacist was like, \\\"it happens all the time\\\". I just thought, \\\"Really? Cause its first tard move for me\\\" ...lol. Don't know if she was just being nice...anywho she called and checked if my insurance would waive the cost for the rx since it had no refills and yada, yada, yada...hehe...long story, short...I cleaned up the 10 days worth of amox off my carpets, couch, bathed my baby, and got a new rx...I'm happy!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4 stars</td>\n      <td>Been there twice for lunch during business hours, service is always fast and friendly, had the same young lady each time, really attentive and nice.\\n\\nHave had their Turducken Burger, California Chicken Club, and Buffalo Chicken Soup... always tastes good and fresh, the soup is def ambitious, but a bit odd.\\n\\nOverall I would recommend for anybody in the area, they are really great about rotating the business lunch crowd and prices are very good</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4 stars</td>\n      <td>I went to Middle School here back in the day. Although those were not the best years of my life all and all I would say DSMS is a great school. Overall I had very passionate, caring, and knowledgeable teachers who I still remember. I was very shy in those days, but now that I think back the school still got me very involved. I did orchestra, babysitters club, community service club, and a program where I aided for a teacher at the neighboring elementary school. Located in a good part of Phx/Scottsdale, the school isn't the prettiest place to look at but that doesn't reflect on the school itself!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1 star</td>\n      <td>Why cant i give them  no stars?!\\nThis is the ost disguting place i have ever eaten at in my entire life.\\nWe ordered the kung poa chicken.\\nI have to admit i really liked the sauce but my mother didint even believe it was chicken.\\nEven after we left she said no way it was chicken. It was nasty and tasted like nothing and looked raw.\\nWe also ordered the chow mein pork and chicken....the chicken was the same and the pork...no one could even eat it..it tasted nasty.\\nMe and my mom ended up eating the rice with hot sauce because we could not eat any of the dishes.\\nThey never asked if we wanted anything to drink.\\nThe served tea at the start which almost made me throw up because it tasted like hot water.\\nI will say that my mom thought it was okay. She drank almost all of it.\\nMy dad was mad that he had to pay so much. It was 60 dollars for 3 plates.\\nthe last plate was chicken and mushrooms..no one even touched that.\\nThe mushrooms were hard to chew and weird tasting and the sauce was gross.\\nMy dad did not eat anything and when we left he went straight to the chinese grocery store to buy some mangos because he was starving.\\nMy mom left hungry and she felt sick all day.\\nNever going back and telling everyone how bad this place is.\\nI've tasted some nasty stuff but i have never had such bad food.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4 stars</td>\n      <td>If this review was to be on the food alone, it'd be a solid Four-Star review.  However, I have to take into account the antiquated decor and the ever present smell of cigars and cigarettes.  The dining room is very dark with red vinyl/pleather seating.  On top of that, the service can be slow at times.\\n\\nThat being said, I make it a point to stop here on every trip to Vegas.  Usually, immediately after getting off of the plane at around 10am Vegas time.  Upon first site, it really isn't much.  Ellis Island is hooked onto a Super 8 Motel; which in itself, doesn't exactly make you think of award winning micro-brewed beer and excellent food.  \\n\\nThis place definitely has the feel of \\\"Old Vegas\\\" when it comes to the $6 Steak (big) and Eggs along side a $1 beer brewed on site.  The menu is wide ranging.  You'll find everything from basic spaghetti to chicken fried steak on the dinner menu and slabs of ham the size of your head for breakfast!\\n\\nThe Ellis Island microbrewery is on site and makes a few excellent brews.  My favorite is the stout...and for $1 you can't go wrong here.  \\n\\nIf you're looking for somewhere cheap (because nothing else in Vegas is!) to start your trip while you wait for the 2-3pm hotel check-in time look no further.  \\n\\nOne last thing, don't make the mistake of having the \\\"Big as your head\\\" ham &amp; eggs just before heading home...it makes for a long plane ride!!</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2 star</td>\n      <td>It's fine for a quick meal, but not someplace I would seek out.\\n\\nCollege students will like it.  It is cheap, hot and there is a variety, but it is nothing special.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3 stars</td>\n      <td>This is a pretty nondescript little sushi spot stuck on in a suburban strip mall.  As others have noted, this isn't a hopping scene by any means.  But if you want consistently good sushi, you can do much worse than this in Charlotte.\\n\\nService is almost always a bit slow, but there is no doubting the friendliness of everyone who works here.\\n\\nIf you have kids and love to eat sushi (especially if you have kids WHO love to eat sushi), this is a good standby.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2 star</td>\n      <td>This review is probably biased because our group took up most of this small space. I expected this to be pretty douchey, but it wasn't that bad. I sat on the floor for a bit and the seating was what made it really great. Beers were 6 bucks, but it's pointless to complain about prices when you go to Vegas; if you're going to complain about drink prices don't hang out at a newly opened, fancy pants hotel. There are several places where you can get a buzz for 10 bucks. Anyway, why the low star count? The music was crap. Not even our server's luscious breasts could make up for hearing Miley Cyrus and The Village People at 3 am. The most serious of WTF's.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3 stars</td>\n      <td>Very clean and organized store- bonus star for having teacher discounts for items used in organizing classrooms! Not sure if this is a special sale due to Teacher Appreciation Week or a discount that can be used throughout the year with proper ID.\\n\\nStore itself is, predictably, organized to the max.\\nFell in love with a pull-out desk, and I don't even NEED a desk! Love the little d\\u00e9cor touches sprinkled throughout the store, making the furniture and displays look much more realistic than the pictures in the catalogs.\\n\\nService seems like it could be stellar- the ELFA section in particular even has light up buzzers like what you get when waiting for your table at a restaurant, which I assume come into play when you're waiting to work with a specialist one on one.\\nHowever, despite wandering the store for nearly half an hour collecting a basket of items to organize my bathroom, I was only ever spoken to by the guy who checked me out as I left.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1 star</td>\n      <td>I hate this airport! It is inefficient, badly organize and the transport links makes Montreal looks worst that developing countries.\\n\\nIt seems that the security and Canadian customs are managed to optimize their own resource without the slightest care about the passengers.\\n\\nSeriously, they made a big investment on the building in the last few years and expanded Canadian custom hall quite a bit... BUT they don't staff it. Schedule plane service landed every day about the same time, can't you staff the place so people don't have to wait an hour or more for immigration? Apparently not. Canada is also the only place I know where citizen and permanent residents don't have a priority line (you know because citizen can be cleared faster and you can clear the place?).\\n\\nThe train line that go to downtown is less than a mile away, but there is no convenient train services to downtown and they have been talking about a rail link to the terminal for over 40 years... nothing\\n\\nCan this place be made efficient and serve it's function well? Yes. Does it do so now? No, absolutely not.</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "MODEL_PATH= \"/mnt/data/huggingface/models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{MODEL_PATH}/google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2 star</td>\n      <td>I stopped at Panda Express during my visit to the Venetian during the holidays. \\nI had never experienced anything like it. There was a young woman texting on her cellphone while I waited for help. Two people waiting for assistance, a gentleman in front of me and I. I am an avid texter so I decided to be patient while she went on texting.\\n   Much to my surprise, a young man arrives (in Panda Express uniform) and starts kissing her. It was more than the average kiss...lol! She turns to the counter and a bit frustrated makes an attempt to take our orders.\\n   By this time, I'm already contemplating just walking away a few more steps to the other food vendors, when a very nice Asian woman comes to the rescue and starts taking our orders. The food was great.\\n   I know it's fast food Panda Express; I do not expect red carpet treatment here but the kissing part was a little extreme lol! For that reason, I am giving it 2 stars.</td>\n      <td>[101, 146, 2141, 1120, 6991, 1810, 5764, 1219, 1139, 3143, 1106, 1103, 16574, 1219, 1103, 13413, 119, 165, 183, 2240, 1125, 1309, 4531, 1625, 1176, 1122, 119, 1247, 1108, 170, 1685, 1590, 3087, 1158, 1113, 1123, 2765, 9293, 1229, 146, 3932, 1111, 1494, 119, 1960, 1234, 2613, 1111, 5052, 117, 170, 12400, 1107, 1524, 1104, 1143, 1105, 146, 119, 146, 1821, 1126, 21976, 3087, 1200, 1177, 146, 1879, 1106, 1129, 5351, 1229, 1131, 1355, 1113, 3087, 1158, 119, 165, 183, 6335, 1106, 1139, 3774, 117, 170, 1685, 1299, 8121, 113, 1107, 6991, 1810, 5764, 6029, 114, 1105, 3816, 7567, 1123, ...]</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 10)\n"
     ]
    }
   ],
   "source": [
    "nnn = range(10)\n",
    "print(nnn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index 99999 out of range for dataset of size 50000.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# 10w\u001B[39;00m\n\u001B[1;32m      8\u001B[0m small_train_dataset \u001B[38;5;241m=\u001B[39m tokenized_datasets[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshuffle(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100000\u001B[39m))\n\u001B[0;32m----> 9\u001B[0m small_eval_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtokenized_datasets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    555\u001B[0m }\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    558\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/fingerprint.py:481\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    477\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[1;32m    479\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[0;32m--> 481\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/arrow_dataset.py:3794\u001B[0m, in \u001B[0;36mDataset.select\u001B[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001B[0m\n\u001B[1;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_range_contiguous(indices) \u001B[38;5;129;01mand\u001B[39;00m indices\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   3793\u001B[0m         start, length \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mstart, indices\u001B[38;5;241m.\u001B[39mstop \u001B[38;5;241m-\u001B[39m indices\u001B[38;5;241m.\u001B[39mstart\n\u001B[0;32m-> 3794\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_select_contiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_fingerprint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_fingerprint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3795\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3796\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    555\u001B[0m }\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    558\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/fingerprint.py:481\u001B[0m, in \u001B[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    477\u001B[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001B[1;32m    479\u001B[0m \u001B[38;5;66;03m# Call actual function\u001B[39;00m\n\u001B[0;32m--> 481\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:  \u001B[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001B[39;00m\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/arrow_dataset.py:3855\u001B[0m, in \u001B[0;36mDataset._select_contiguous\u001B[0;34m(self, start, length, new_fingerprint)\u001B[0m\n\u001B[1;32m   3852\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   3854\u001B[0m _check_valid_indices_value(start, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n\u001B[0;32m-> 3855\u001B[0m \u001B[43m_check_valid_indices_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3856\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m length \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   3857\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Dataset(\n\u001B[1;32m   3858\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mslice(start, length),\n\u001B[1;32m   3859\u001B[0m         info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mcopy(),\n\u001B[1;32m   3860\u001B[0m         split\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit,\n\u001B[1;32m   3861\u001B[0m         fingerprint\u001B[38;5;241m=\u001B[39mnew_fingerprint,\n\u001B[1;32m   3862\u001B[0m     )\n",
      "File \u001B[0;32m/mnt/data/software/miniforge3/envs/peft/lib/python3.10/site-packages/datasets/arrow_dataset.py:649\u001B[0m, in \u001B[0;36m_check_valid_indices_value\u001B[0;34m(index, size)\u001B[0m\n\u001B[1;32m    647\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_valid_indices_value\u001B[39m(index, size):\n\u001B[1;32m    648\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (index \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m index \u001B[38;5;241m+\u001B[39m size \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m size):\n\u001B[0;32m--> 649\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m out of range for dataset of size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: Index 99999 out of range for dataset of size 50000."
     ]
    }
   ],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# 10w\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/data/huggingface/models/google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"{MODEL_PATH}/google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# model_dir = \"models/bert-base-cased-finetune-yelp\"\n",
    "model_dir = f\"{MODEL_PATH}/google-bert/bert-base-cased-finetune-yelp\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/data/huggingface/models/google-bert/bert-base-cased-finetune-yelp/runs/May29_15-36-46_shjscs2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/mnt/data/huggingface/models/google-bert/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/data/huggingface/models/google-bert/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。\n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "SPACE_PATH = \"/mnt/data/huggingface/spaces\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "metric = evaluate.load(f\"{SPACE_PATH}/evaluate-metric/accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_dir,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_eval_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      2\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      3\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m      4\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39msmall_train_dataset,\n\u001B[0;32m----> 5\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39m\u001B[43msmall_eval_dataset\u001B[49m,\n\u001B[1;32m      6\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m      7\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'small_eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39mtrain()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e30510-0536-49d4-8e1b-43fc25272bde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9441ad-f65a-42b7-9016-4809c78285e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92e35d-fed7-4ff2-aa84-27b5e29b917a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2580a-7a5a-46ae-a28b-b41e9e838eb1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}